# -*- coding: utf-8 -*-
"""Proyecto 1 Inteligencia Artificial.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iojvbluqFgTIv7VVJYDTpDcRRqEY5k1B

# I. Stock price predictions using regressions.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

###########Data for lineal regression
df = pd.read_csv("/NSE-TATAGLOBAL11.csv")

df = df.sort_index(ascending=False) # Invertir orden de filas
df['Day'] = range(1, len(df) + 1)   # Crear columna "Día" para X

df.head(n=1235)
#df.tail(n=1235)

class Graph:

    def plotTitleAndLabels(self, title, xlabel, ylabel):
        plt.title(title)
        plt.xlabel(xlabel)
        plt.ylabel(ylabel)

    def plotScatter(self, X, Y, size, color):
        plt.scatter(X, Y, s = size, c = color);

    def plotPolynomialRegression(self, a, b, X, Y, size, color, totalX, deg):
        X = np.linspace(min(totalX), max(totalX), 1000)
        if (deg == 1):
            Y = b[0]*X + a
        if (deg == 2):
            Y = b[1]*X**2 + b[0]*X + a
        if (deg == 3):
            Y = b[2]*X**3 + b[1]*X**2 + b[0]*X + a
        if (deg == 4):
            Y = b[3]*X**4 + b[2]*X**3 + b[1]*X**2 + b[0]*X + a
        if (deg == 5):
            Y = b[4]*X**5 + b[3]*X**4 + b[2]*X**3 + b[1]*X**2 + b[0]*X + a
        if (deg == 6):
            Y = b[5]*X**6 + b[4]*X**5 + b[3]*X**4 + b[2]*X**3 + b[1]*X**2 + b[0]*X + a
        if (deg == 7):
            Y = b[6]*X**7 + b[5]*X**6 + b[4]*X**5 + b[3]*X**4 + b[2]*X**3 + b[1]*X**2 + b[0]*X + a
        if (deg == 8):
            Y = b[7]*X**8 + b[6]*X**7 + b[5]*X**6 + b[4]*X**5 + b[3]*X**4 + b[2]*X**3 + b[1]*X**2 + b[0]*X + a
        if (deg == 9):
            Y = b[8]*X**9 + b[7]*X**8 + b[6]*X**7 + b[5]*X**6 + b[4]*X**5 + b[3]*X**4 + b[2]*X**3 + b[1]*X**2 + b[0]*X + a
        plt.scatter(X, Y, s = size, c = color)

class Aux:

    def numpyCoefficients(self, B, deg):
        b = np.zeros(deg)
        for i in range (deg):
            b[i] = B[:,i]
        return b

"""### Regresión lineal y polinomial"""

def main():
    
    # Abrir dataframe, invertir orden y crear nueva columna "Día" para el vector X
    df = pd.read_csv("/NSE-TATAGLOBAL11.csv")
    df = df.sort_index(ascending=False)
    df['Day'] = range(1, len(df) + 1)

    # Porcentaje de datos de entrenamiento (70%)
    train_p = 0.7

    # Grado de la regresión
    deg = 9

    # Parámetros para iteración
    tol = 1
    maxIter = 50
    error = np.inf
    iter = 0
    
    while (iter < maxIter and tol < error ):

        # Definición de variables
        X = df['Day']
        Y = df['Close']

        # Convertir listas en arreglos numpy y luego convertir estos en columnas
        X = np.c_[X.to_numpy()]
        Y = np.c_[Y.to_numpy()]
        
        # Obtener datos de entrenamiento (70%) y de prueba (30%)
        x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = (1-train_p))

        # Obtener el cuadrado de cada valor de x en los datos
        poly_feat = PolynomialFeatures(degree = deg, include_bias = False)
        x_train_poly = poly_feat.fit_transform(x_train)
        x_test_poly = poly_feat.fit_transform(x_test)

        # Regresión lineal con datos de entrenamiento
        model = LinearRegression()
        model.fit(x_train_poly, y_train)
        a = model.intercept_
        B = model.coef_
        objA = Aux()#objeto para reordenar coeficientes
        b = objA.numpyCoefficients(B, deg)

        # Predicciones del modelo usando los datos de prueba
        y_prediction = model.predict(x_test_poly)
        
        # Error cuadrático medio entre la predicción y el valor real de los datos de prueba
        MSEtest = mean_squared_error(y_test, y_prediction)
        
        # Validación cruzada
        if (MSEtest < error):
            error = MSEtest
            best_a = a
            best_b = b
            best_x_train = x_train
            best_y_train = y_train
            best_x_test = x_test
            best_y_test = y_test
            best_x_test_poly = x_test_poly
            best_y_prediction = y_prediction

        # Iteración
        iter += 1
    
    # Error mínimo
    print("DEGREE:", deg)
    print("Error: ", error)
    print("Alpha: ", best_a)
    print("Beta:  ", best_b, "\n")
    
    # Graficar
    objG = Graph()
    objG.plotTitleAndLabels("TGB Stock Prices Regression", "Time (Days)", "Stock Prices (USD)")
    objG.plotScatter(best_x_train, best_y_train, 1, "blue")  # Datos de entrenamiento
    objG.plotScatter(best_x_test, best_y_test, 1, "red")     # Datos de prueba
    objG.plotPolynomialRegression(best_a, best_b, best_x_test_poly[:,0], best_y_prediction, 1, "red", X, deg)  # Regresión lineal
    plt.show()

if __name__ == "__main__":
    main()

"""# II. Stock clustering using K-Means algorithm."""

#Libraries
import pandas as pd
import numpy as np
import random as rd
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
import math
import pylab as plt
import pandas_datareader as dr  
from scipy.cluster.vq import kmeans, vq
from time import time

"""##### You only have to run this code once to download the data from yahoo
##### You have to download the data from https://www.quandl.com/data/NSE/TATAGLOBAL-Tata-Global-Beverages-Limited and then you can upload it to your google drive account to run this code

#*********************Import data for the K-Means clustering********************************
#*******************************************************************************************
SP_url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'
table = pd.read_html(SP_url)#remember that the link imports two tables, we only need the Symbols of the companies
                              #who are in the first table
symbols = table[0][1:]['Symbol'].values #Tickers of the companies
#we do not have the stock prices of the companies, so we search on yahoo for each symbol
prices_data=[] #vector that will contain the prices of the stocks
source = 'yahoo'
start_date = '31/03/2020'
end_date = '31/03/2021'
for symbol in symbols:
  #lets use the "try" statement to handle the exceptions
  try:
    prices = dr.DataReader(symbol, source, start_date, end_date)['Adj Close']
    #The adjusted closing price amends a stock's closing price to reflect that stock's value after accounting for any corporate actions
    prices = pd.DataFrame(prices)#convert to dataframe
    prices.columns = [symbol]
    prices_data.append(prices)#append the prices of the actual stock to the vector containing all prices of all stocks
  except:
    pass #null operation
  
  prices_df = pd.concat(prices_data, axis=1)

prices_df.sort_index(inplace =True) #add index

#saving the DataFrame so we do not have to run this code each time
to_save = prices_df
to_save.to_csv('stockPrices.csv')
"""

#Import data
prices_df = pd.read_csv('/content/stockPrices (3).csv')
prices_new = pd.read_csv('/content/stockPrices (3).csv')
prices_new.pop('Date')

class KMeansAlgo:
  
  #******************************Implementation of the K-Means Algorithm from Scratch***************************************
  #******************************************************************************************************************
  def KMeans_implementation(self,X, column_1, column_2, k):
    start_time = time()
    #################### Step 1, n_clusters,. predefined####################################
    ######################################################################################
    ####################### Step 2, select centroids#############################
    ######################################################################################3
    Centroids = (X.sample(n=k))#random centroids 

    ################### Step 3, each point to the closest centroid##########################
    #######################################################################################
    diff = 1 #variable to store the distance between centroids in the previous and the actual iteration
    j = 0 #label to validate diff in the first iteration
    print("******************************************************************************************")
    print("\n\n\nK-Means Implementation from Scratch")
    print("\nDistance from old centroids to new Centroids")
    while(diff != 0 ): #condition to stop
      X_1 = X #we will modificate X so we have to save another variable with exactly the same value
      i = 1 #centroid position
      for index1,row_1 in Centroids.iterrows(): #saving indexes and values(x and y axis) of Centroids, iterating on each centroid
        cent_dist = [] #here we will save the value of the distances from each point to each centroid so we can compare later
        for index2,row_2 in X_1.iterrows():#now we iterate on each point
          dist_1 = (row_1[column_1] - row_2[column_1])**2 #we have to put the desired column here
          dist_2 = (row_1[column_2]-row_2[column_2])**2#next column
          distance = np.sqrt(dist_1 + dist_2)#pitagoras
          cent_dist.append(distance)#saving the vector of distances
        X[i] = cent_dist  #add a column of distances to each centroid
        i+=1 #change of centroid

      best_cluster = [] 
      for index, row in X.iterrows(): #now we can iterate the distances to each centroid
        min_dist = row[1] #initializing minimum distance 
        pos = 1
        #compare the distances and update
        for i in range(k):
          if(row[i+1] < min_dist):#it's i+1 because the row index is at 1
            min_dist = row[i+1]
            pos = i+1
        best_cluster.append(pos) #here we save the value(number of cluster) with the closest centroid to the point
      X["Cluster"] = best_cluster

      #########################Step 4.	Recompute the centroids of newly formed clusters.#############
      ##############################################################################################
      updated_Centroid = X.groupby(["Cluster"]).mean()[[column_1, column_2]]
      if(j==0):#first iteration
        diff = 1
        j+=1
      else:
        diff = (updated_Centroid[column_1]-Centroids[column_1]).sum()+(updated_Centroid[column_2]-Centroids[column_2]).sum()
        print(diff.sum())
      Centroids = X.groupby("Cluster").mean()[[column_1,column_2]]
    print("\nCentroids: \n", updated_Centroid)
    print("Elapsed Time: \n", time()-start_time)
    return Centroids 

  ####################Plot K-Means Implementation##############################################
  def plot(self, Centroids,k,X, column_1, column_2):
    fig = plt.figure(figsize=(13,8))
    #Centroids = KMeans_implementation()
    color = ['brown','gray','black','cyan','violet','gold']
    for clust in range(k):#iterate in each cluster
      nCluster = X[X["Cluster"] == clust+1]#number of cluster
      plt.scatter(nCluster[column_1],nCluster[column_2],c = color[clust])
    plt.scatter(Centroids[column_1], Centroids[column_2],s=100, c = 'red', marker="*")
    plt.title("K-Means Algorithm From Scratch")
    plt.xlabel(column_1)
    plt.ylabel(column_2)
    #plt.show()
    fig.savefig('img_1.jpg', bbox_inches='tight', dpi=300)
  #******************Plotting the Elbow Curve to Choose The number of Clusters***************************************
  #******************************************************************************************************************
  def elbow_Curve(self):
    #average annual percentage return and volatilities
    ret_vol = prices_new.pct_change().mean()*252 #annual return --> mean= (sum of all values)/(total of values)
                                                 
    print("Original Data Extracted from the Web\n \n", prices_df, "\n\n")
    print("Percentage Change Between the Current and a Prior Element for each Stock\n\n", prices_new.pct_change()*100, "\n\n")
    print("Annual Return")    
    print("There are 253 days of negociation")
    print("So we have 252 data to calculate the Annual Return for each Stock")    
    print(ret_vol, "\n\n")
    ret_vol = pd.DataFrame(ret_vol) #convert to dataframe
    ret_vol.columns = ["Returns"]#set the name of the column 
    #The calculation of historical volatility in the stock market is summarized as the standard deviation of the historical variations in an asset 
    ret_vol['Volatility'] = prices_new.pct_change().std()*math.sqrt(252) #risk=volatility
                                                                        #we can measure the volatility with the standard deviation
    print("Volatility:")    
    print("The calculation of historical volatility in the stock market \nis calculated using the standard deviation of the historical variations of an asset. ")                                                                     
    print("The risk is directly related to the volatility\n")
    print(prices_new.pct_change().std()*math.sqrt(252))
    #convert the data as an array to use it into the KMeans_with_scikitlearn function
    data = np.asarray([np.asarray(ret_vol["Returns"]), np.asarray(ret_vol["Volatility"])]).T #Transpose of the new array containing the 
                                                                                             # Returns and Volatility variables (vector of two columns)
    x = data #(n_samples, n_features)--> 502 x 2 in this case
    SSE = []
    total_clusters = np.arange(2,25)
    for cluster_num in total_clusters:
      k_means = KMeans(n_clusters=cluster_num)
      k_means.fit(x)
      inertia = k_means.inertia_
      SSE.append(inertia)

    fig_1 = plt.figure(figsize=(12,12))
    plt.subplot(2,1,1)
    plt.plot(total_clusters, SSE)
    plt.grid(True)
    plt.title("Elbow Curve")
    plt.xlabel("No. of Clusters")
    plt.ylabel("Sum Squared Error (SSE)")
    print("\n\nWe can see that the optimal number of clusters is between 4 and 6, let's choose 5")
    return data, ret_vol,fig_1
  #********************Implementation of the K-Means Algorithm Using the Scikit Learn Package***************************************
  #******************************************************************************************************************
  def KMeans_with_scikitlearn(self, clusters, dat,fig_1):
    start_time = time()
    centroids, dist = kmeans(dat, clusters)#k-means with  vectors in data and K clusters 
    index, distances= vq(dat, centroids)#returns the index for each observation and the distance between the observation
                            #and its nearest centroid
    print("Elapsed Time: \n", time()-start_time)
    plt.subplot(2,1,2)
    plt.plot(dat[index==0,0], dat[index==0,1], 'ok',
             dat[index==1,0], dat[index==1,1], 'ob',
             dat[index==2,0], dat[index==2,1], 'og',
             dat[index==3,0], dat[index==3,1], 'oy',
             dat[index==4,0], dat[index==4,1], 'oc', markersize = 6)
    plt.plot(centroids[:,0],centroids[:,1],'*r',markersize=8)
    plt.title("K-Means Algorithm Using Scikit-Learn")
    plt.xlabel("Returns")#mean of the percentage change between the current and a prior element*200
    plt.ylabel("Volatility")
    #plt.show()
    fig_1.savefig('img_2.jpg', bbox_inches='tight', dpi=300)
    print("Centroids:\n", centroids)
    print("Data: \n", dat)
    print(len(dat))


  #*********************Implementation of Linear Regression Algorithm for Stock Price Prediction***************************************
  #******************************************************************************************************************
  #def LinearRegression(self):



def main():
  #class as an object
  km = KMeansAlgo()
  #
  dat,ret_vol,fig_1 = km.elbow_Curve()
  clusters = 5 #we choose 5 as the number of clusters based on the elbow curve method
  km.KMeans_with_scikitlearn(clusters, dat,fig_1)
  X = ret_vol[["Returns", "Volatility"]]
  X = pd.DataFrame(X)
  Centroids = km.KMeans_implementation(X, "Returns", "Volatility", clusters)
  km.plot(Centroids,clusters,X, "Returns", "Volatility")
if __name__=="__main__": 
  main()

